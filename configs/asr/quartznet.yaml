project_name: asr_quartznet

# The models are trained using NovoGrad optimizer [26] with a cosine annealing learning rate policy.
# We also found that learn- ing rate warmup helps stabilize early training.

# Use mixed precision training.

# Ранить subprocess datasets.sh, если датасеты не загружены

# Сделать еще Cutout аугментацию, а не только freq и time masking

# УБРАТЬ mr, ms, etc. как прописано в LJ Speech!!!
# ['mr neild was compelled to admit in eighteen twelve that the great reformation produced by howard',

model:
  pretrained: False
  checkpoint_path: null
  path_to_save: null

  downsize: 2
  name: QuartzNet
  args:
    B: 5  # number of QuartzBlocks
    S: 3  # number of QuartzBlock repeats
    R: 5  # number of subblocks within QuartzBlock
    initial_hidden_channels: 256 # must be devisible by n_transform
    block_hidden_channels:
      - 256,256
      - 256,512
      - 512,512
      - 512,512
      - 512,512
    block_kernel_sizes:
      - 33
      - 39
      - 51
      - 63
      - 75

  normalization:
    name: BatchNorm1d
    args: null

  activation:
    name: ReLU
    args: null

training:
  seed: 42
  epochs: 1_000_000
  n_gpu: 1  # number of gpus, 0 for CPU

  grad_clip_val: 10.0
  log_every_n_steps: 1

  optimizer:
    name: NovoGradWrapper
    args:
      lr: 0.05
      betas:
        - 0.95
        - 0.5
      eps: 1e-8
      weight_decay: 0.001
      grad_averaging: false
      amsgrad: false

  scheduler:
    name: CosineAnnealingWarmupRestartsWrapper
    args:
      first_cycle_steps: 300_000
      cycle_mult: 1.0
      max_lr: 0.05
      min_lr: 0.001
      warmup_steps: 1000
      gamma: 1.0

data:
  max_text_length: null  # in characters
  max_audio_duration: null  # in seconds
  limit: null  # limit the size of a dataset

  use_aug: True

  batch:
    drop_last: true  # whether to drop the last incomplete batch, if the dataset size is not divisible by the batch size
    num_workers: 0
    shuffle: false
    batch_size: 8
    sampler: null

  parts:
    train:
      proportion: 0.8
      datasets:
        - LJSpeechDataset,train
        #- LibriSpeechDataset,train-clean

    test:
      proportion: 0.15
      datasets:
        - LJSpeechDataset,test
        #- LibriSpeechDataset,test-clean

    val:
      proportion: 0.05
      datasets:
        - LJSpeechDataset,val
        #- LibriSpeechDataset,dev-clean

preprocess:

  text:
    encoder:
      name: BaseTextEncoder

  audio:
    sr: 16000  # sample rate
    frequency_min: 0
    frequency_max: 8000  # above 6 kHz sounds become like whines and whistles because they are high pitched

    augmentation:
      use_room_reverberation: True
      use_background_noise: True
      snr_dbs:  # How much audio signal can be greater than noise in dB
        - 20
        - 10
        - 3
      use_sox_effects: True
      effects:
        tempo: 0.2  # max absolute difference in adjusting tempo (but not pitch)
        pitch: 50  # max absolute difference in adjusting pitch (but not tempo)
        speed: 0.2  # max absolute difference in adjusting speed (pitch and tempo together)

  transform:
    name: MelSpectrogram
    n_transform: 64 # n_mels or n_mfcc depending on transformation used
                    # initial_hidden_channels must be devisible by it

    win_length_ms: 30  # window length in milliseconds for FFT
    hop_length_ms: 5  # window step in milliseconds for FFT

    melspectrogram:
      n_mels: 64  # mumber of mel filterbanks
      mel_type: htk  # mel = 2595.0 * np.log10(1.0 + f / 700.0)

    mfcc:
      n_mfcc: 40  # number of mfc coefficients to retain
      dct_type: 2  # DCT type to use
      log_mels: False # whether to use log-mel spectrograms instead of db-scaled

    augmentation:
      feature_time_mask_prob: 0.8  # probability of using feature & time masking
      feature_mask_prob: 0.6  # probability of using feature masking if masking is allowed
      time_mask_prob: 0.6  # probability of using time masking if masking is allowed
      n_feature_masks: 2  # maximum number of feature masks
      n_time_masks: 2  # maximum number of time masks
      feature_portion: 0.15  # maximum portion of a single feature mask
      time_portion: 0.15  # maximum portion of of a single time mask
